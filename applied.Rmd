---
title: "STAT 455 Project: Forecasting U.S. Oil Rig Counts"
author: "Time Series Analysis"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: cosmo
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  fig.align = "center"
)
```

# 1. Introduction

This project compares ARIMA and TSGLM (ACP/INGARCH) models for forecasting monthly U.S. oil rig counts. We evaluate model performance using rolling-window out-of-sample forecasts and report MAE, MAPE, and RMSE metrics.

# 2. Data Preparation

## 2.1 Load Libraries

```{r libraries}
library(ggplot2)
library(forecast)
library(zoo)
library(tscount)
library(dplyr)
library(tidyr)
library(knitr)
```

## 2.2 Load Data

```{r load-data}
data <- read.csv('data/rigs.csv')

data$date <- as.Date(paste0("01-", data$date), format = "%d-%b-%y")
```

## 2.3 Visualize Raw Data

```{r plot-raw-data}
p_raw <- ggplot(data, aes(x = date, y = count)) +
  geom_line(color = "steelblue", linewidth = 0.8) +
  geom_point(color = "darkblue", linewidth = 0.5, alpha = 0.5) +
  theme_minimal(base_size = 14) +
  labs(
    title = "U.S. Oil Rig Count Over Time",
    x = "Date",
    y = "Number of Rigs",
    caption = "Source: U.S. Energy Information Administration"
  ) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.caption = element_text(face = "italic")
  )

print(p_raw)
```

## 2.4 Convert to Time Series Object

```{r convert-ts}
data$date <- as.yearmon(data$date, "%b-%y")

z <- ts(data$count, 
        start = c(as.numeric(format(data$date[1], "%Y")), 
                  as.numeric(format(data$date[1], "%m"))),
        frequency = 12)
```

# 3. Experimental Setup

```{r setup-parameters}
# Test set configuration (5 years)
test_years <- 5
test_horizon <- test_years * 12

# Time series parameters
n <- length(z)
start_test <- n - test_horizon
h <- 1  # 1-step ahead forecast
dates <- time(z)
```

# 4. Exploratory Data Analysis

## 4.1 ACF and PACF Plots

```{r acf-pacf}
acf(as.numeric(z)[1:start_test], 
    main = "ACF of Training Data",
    col = "darkred",
    lwd = 2)

pacf(as.numeric(z)[1:start_test], 
     main = "PACF of Training Data",
     col = "darkblue",
     lwd = 2)
```

# 5. Model Evaluation Framework

## 5.1 Evaluation Metrics Function

```{r eval-function}
evaluate_metrics <- function(actual, forecasted) {
  acc <- accuracy(forecasted, actual)
  
  data.frame(
    RMSE = round(acc["Test set", "RMSE"], 2),
    MAE  = round(acc["Test set", "MAE"], 2),
    MAPE = round(acc["Test set", "MAPE"], 2)
  )
}
```

# 6. Model Implementation

## 6.1 Define Models

```{r define-models}
arima_models <- list(
  AR1    = c(1, 1, 0),     # ARIMA(1,1,0)
  MA1    = c(0, 1, 1),     # ARIMA(0,1,1)
  ARMA11 = c(1, 1, 1),     # ARIMA(1,1,1)
  AR3 = c(3, 1, 0),      # ARIMA(3,1,0)
  AR5 = c(5, 1, 0)      # ARIMA(5,1,0)
)

# TSGLM model specifications
tsglm_models <- list(
  AR1    = list(past_mean = NULL, past_obs = 1),      # ACP(1,0)
  MA1    = list(past_mean = 1, past_obs = NULL),      # ACP(0,1)
  ARMA11 = list(past_obs = 1, past_mean = 1),         # ACP(1,1)
  AR3 = list(past_obs = 3, past_mean = NULL),       # ACP(3,0)
  AR5 = list(past_obs = 5, past_mean = NULL)       # ACP(5,0)
  
)

# Scale data for TSGLM (to avoid numerical issues)
z_scaled <- z / 100
window_size <- 36  # Default sliding window size
```

## 6.2 Rolling Window Forecasting Function

```{r forecast-function}
rolling_forecast <- function(model_type = "ARIMA", model_name, model_spec) {
  # Initialize prediction vector
  pred <- rep(NA, n)
  
  if (model_type == "ARIMA") {
    # ARIMA rolling forecasts
    for (i in start_test:(n - h)) {
      train <- window(z, end = time(z)[i])
      fit <- tryCatch(
        Arima(train, order = model_spec),
        error = function(e) NULL
      )
      if (!is.null(fit)) {
        pred[i + h] <- forecast(fit, h = h)$mean[h]
      }
    }
    
  } else if (model_type == "TSGLM") {
    # TSGLM rolling forecasts
    max_lag <- max(unlist(model_spec), na.rm = TRUE)
    start <- max(start_test, max_lag + 5)
    
    for (i in start:(n - h)) {
      train <- z_scaled[(i - window_size + 1):i]
      fit <- suppressWarnings(
        tryCatch(
          tsglm(train, 
                model = model_spec,
                link = "identity", 
                distr = "poisson",
                init.method = "marginal", 
                init.drop = FALSE),
          error = function(e) NULL
        )
      )
      if (!is.null(fit)) {
        lambda_next <- predict(fit, n.ahead = h)$pred[h]
        pred[i + h] <- lambda_next * 100  # Rescale back
      }
    }
  }
  
  return(pred)
}
```

# 7. Model Training and Evaluation

## 7.1 Run ARIMA Models

```{r run-arima}
cat("Running ARIMA models...\n")
arima_results <- list()
arima_predictions <- list()

for (name in names(arima_models)) {
  cat("  - Fitting ARIMA", name, "\n")
  
  # Get predictions
  pred <- rolling_forecast("ARIMA", name, arima_models[[name]])
  arima_predictions[[name]] <- pred
  
  # Calculate metrics
  idx <- start_test + h
  actual <- z[idx:n]
  forecasted <- pred[idx:n]
  valid <- !is.na(forecasted)
  
  if (sum(valid) > 0) {
    metrics <- evaluate_metrics(actual[valid], forecasted[valid])
    metrics$Model <- paste0("ARIMA_", name)
    metrics$Type <- "ARIMA"
    arima_results[[name]] <- metrics
  }
}
```

## 7.2 Run TSGLM Models

```{r run-tsglm}
cat("\nRunning TSGLM models...\n")
tsglm_results <- list()
tsglm_predictions <- list()

for (name in names(tsglm_models)) {
  cat("  - Fitting TSGLM", name, "\n")
  
  # Get predictions
  pred <- rolling_forecast("TSGLM", name, tsglm_models[[name]])
  tsglm_predictions[[name]] <- pred
  
  # Calculate metrics
  valid_idx <- which(!is.na(pred))
  
  if (length(valid_idx) > 0) {
    aligned_actual <- z[valid_idx]
    aligned_forecast <- pred[valid_idx]
    
    metrics <- evaluate_metrics(aligned_actual, aligned_forecast)
    metrics$Model <- paste0("TSGLM_", name)
    metrics$Type <- "TSGLM"
    tsglm_results[[name]] <- metrics
  }
}
```

# 8. Results Comparison

## 8.1 Combine All Results

```{r combine-results}
# Combine all results
all_results <- bind_rows(
  bind_rows(arima_results),
  bind_rows(tsglm_results)
) %>% 
  select(Model, Type, RMSE, MAE, MAPE) %>% 
  arrange(RMSE)

all_results
```



# 8. Sensitivity Analysis

## 8.1 Window Size Sensitivity for TSGLM

```{r window-sensitivity}
window_sizes <- list(
  w24 = 24,
  w36 = 36,
  w48 = 48,
  w60 = 60,
  w120 = 120,
  full = NA
)

window_results <- list()

cat("Testing window size sensitivity...\n")
for (wname in names(window_sizes)) {
  w <- window_sizes[[wname]]
  
  for (mname in names(tsglm_models)) {
    pred <- rep(NA, n)
    model_spec <- tsglm_models[[mname]]
    max_lag <- max(unlist(model_spec), na.rm = TRUE)
    start <- max(start_test, max_lag + 5)
    
    for (i in start:(n - h)) {
      if (is.na(w)) {
        train <- z_scaled[1:i]
      } else {
        train <- z_scaled[(i - w + 1):i]
      }
      
      fit <- suppressWarnings(
        tryCatch(
          tsglm(train, 
                model = model_spec,
                link = "identity", 
                distr = "poisson",
                init.method = "marginal", 
                init.drop = FALSE),
          error = function(e) NULL
        )
      )
      
      if (!is.null(fit)) {
        lambda_next <- predict(fit, n.ahead = h)$pred[h]
        pred[i + h] <- lambda_next * 100
      }
    }
    
    valid_idx <- which(!is.na(pred))
    if (length(valid_idx) > 0) {
      metrics <- evaluate_metrics(z[valid_idx], pred[valid_idx])
      metrics$Window <- wname
      metrics$Model <- paste0("TSGLM_", mname)
      window_results[[paste0(wname, "_", mname)]] <- metrics
    }
  }
}

# Display window sensitivity results
window_table <- bind_rows(window_results) %>% 
  select(Window, Model, RMSE, MAE, MAPE) %>% 
  arrange(Model, RMSE)

window_table
```


## 8.2 Training Sample Size Sensitivity for ARIMA

```{r sample-size-sensitivity}
# Create 35-year subset
z_35y <- tail(z, 35 * 12)
n_35y <- length(z_35y)
start_test_35y <- n_35y - test_horizon

size_results <- list()

cat("\nTesting training sample size sensitivity for ARIMA...\n")
for (name in names(arima_models)) {
  # Full series
  pred_full <- rolling_forecast("ARIMA", name, arima_models[[name]])
  actual_full <- z[(start_test + h):n]
  forecasted_full <- pred_full[(start_test + h):n]
  valid_full <- !is.na(forecasted_full)
  
  metrics_full <- evaluate_metrics(actual_full[valid_full], forecasted_full[valid_full])
  metrics_full$Model <- paste0("ARIMA_", name)
  metrics_full$TrainSize <- "Full"
  
  # 35-year series
  pred_35y <- rep(NA, n_35y)
  for (i in start_test_35y:(n_35y - h)) {
    train <- window(z_35y, end = time(z_35y)[i])
    fit <- tryCatch(Arima(train, order = arima_models[[name]]), error = function(e) NULL)
    if (!is.null(fit)) pred_35y[i + h] <- forecast(fit, h = h)$mean[h]
  }
  
  actual_35y <- z_35y[(start_test_35y + h):n_35y]
  forecasted_35y <- pred_35y[(start_test_35y + h):n_35y]
  valid_35y <- !is.na(forecasted_35y)
  
  metrics_35y <- evaluate_metrics(actual_35y[valid_35y], forecasted_35y[valid_35y])
  metrics_35y$Model <- paste0("ARIMA_", name)
  metrics_35y$TrainSize <- "35 Years"
  
  size_results[[paste0(name, "_full")]] <- metrics_full
  size_results[[paste0(name, "_35y")]] <- metrics_35y
}

# Display size sensitivity results
size_table <- bind_rows(size_results) %>% 
  select(Model, TrainSize, RMSE, MAE, MAPE) %>% 
  arrange(Model, TrainSize)

size_table
```

# 9. Best Model Analysis

## 9.1 Identify Best Model

```{r best-model}
best_model <- all_results %>% 
  filter(RMSE == min(RMSE, na.rm = TRUE))

cat("Best Performing Model:\n")
cat("Model:", best_model$Model, "\n")
cat("RMSE:", best_model$RMSE, "\n")
cat("MAE:", best_model$MAE, "\n")
cat("MAPE:", best_model$MAPE, "%\n")
```

## 9.2 Forecast Plot for Best Model

```{r best-model-plot}
# Determine which model type is best
if (grepl("ARIMA", best_model$Model)) {
  model_type <- "ARIMA"
  model_name <- gsub("ARIMA_", "", best_model$Model)
  pred_best <- arima_predictions[[model_name]]
} else {
  model_type <- "TSGLM"
  model_name <- gsub("TSGLM_", "", best_model$Model)
  pred_best <- tsglm_predictions[[model_name]]
}

# Create forecast vs actual plot
forecast_df <- data.frame(
  Date = dates[(start_test + h):n],
  Actual = z[(start_test + h):n],
  Forecast = pred_best[(start_test + h):n]
) %>% 
  filter(!is.na(Forecast))

best_plot <- ggplot(forecast_df, aes(x = Date)) +
  geom_line(aes(y = Actual, color = "Actual"), size = 0.8) +
  geom_line(aes(y = Forecast, color = "Forecast"), size = 0.8, linetype = "dashed") +
  geom_ribbon(aes(ymin = pmin(Actual, Forecast), 
                  ymax = pmax(Actual, Forecast)), 
              alpha = 0.2, fill = "gray") +
  scale_color_manual(
    values = c("Actual" = "#2c3e50", "Forecast" = "#e74c3c"),
    name = ""
  ) +
  theme_minimal(base_size = 14) +
  labs(
    title = paste("Best Model Forecast:", best_model$Model),
    subtitle = paste("RMSE =", best_model$RMSE, "| MAPE =", best_model$MAPE, "%"),
    x = "Date",
    y = "Rig Count"
  ) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "bottom"
  )

print(best_plot)
```